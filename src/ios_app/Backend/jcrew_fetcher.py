"""
J.Crew Product Fetcher - Real-time product data fetching and caching
"""

import requests
from bs4 import BeautifulSoup
import re
import json
from typing import Dict, Optional
import psycopg2
import sys
import os

# Add parent directory to path to import db_config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
from db_config import DB_CONFIG

class JCrewProductFetcher:
    """Fetch J.Crew product data on-demand and cache it"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
    
    def _extract_product_code(self, product_url: str) -> Optional[str]:
        """
        Extract product code from J.Crew URL for consistent caching
        Examples:
        - /p/mens/.../CL752?color=white -> CL752
        - /p/mens/.../BE996 -> BE996
        """
        import re
        
        # Pattern to match J.Crew product codes (usually 5-6 alphanumeric characters)
        # They appear at the end of the path before query parameters
        pattern = r'/([A-Z0-9]{4,6})(?:\?|$)'
        match = re.search(pattern, product_url)
        
        if match:
            return match.group(1)
        
        # Fallback: try to extract from the last segment of the path
        try:
            from urllib.parse import urlparse
            parsed = urlparse(product_url)
            path_segments = [seg for seg in parsed.path.split('/') if seg]
            
            # Look for product code pattern in last few segments
            for segment in reversed(path_segments[-3:]):
                if re.match(r'^[A-Z0-9]{4,6}$', segment):
                    return segment
        except:
            pass
        
        return None
    
    def _normalize_url_for_caching(self, product_url: str) -> str:
        """
        Create a normalized cache key based on product code and color
        This ensures different color variants have separate cache entries
        """
        product_code = self._extract_product_code(product_url)
        
        # Extract color information from URL
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(product_url)
        query_params = parse_qs(parsed.query)
        
        color_info = ""
        if 'color_name' in query_params:
            color_info = f"_{query_params['color_name'][0]}"
        elif 'colorProductCode' in query_params:
            color_info = f"_{query_params['colorProductCode'][0]}"
        
        if product_code:
            return f"jcrew_product_{product_code}{color_info}"
        
        # Fallback to URL-based key (keep color and fit parameters)
        base_path = parsed.path
        filtered_params = {}
        
        # Keep essential parameters including color and fit
        keep_params = ['fit', 'color_name', 'colorProductCode']
        for param in keep_params:
            if param in query_params:
                filtered_params[param] = query_params[param]
        
        # Create normalized cache key
        cache_key = base_path
        if filtered_params:
            param_str = '&'.join(f"{k}={v[0]}" for k, v in filtered_params.items())
            cache_key += f"?{param_str}"
        
        return f"jcrew_url_{hash(cache_key)}"
    
    def fetch_product(self, product_url: str) -> Optional[Dict]:
        """
        Fetch product data from J.Crew URL
        Returns product info or None if failed
        """
        # Check if it's a supported product type (all men's tops including outerwear)
        if not self._is_supported_product(product_url):
            print(f"‚ùå Unsupported product type. Only J.Crew men's tops (shirts, sweaters, jackets) are supported.")
            return None
        
        # Get normalized cache key for consistent caching across color variants
        cache_key = self._normalize_url_for_caching(product_url)
        product_code = self._extract_product_code(product_url)
        
        # First check cache using normalized key
        cached = self._check_cache_by_key(cache_key)
        if cached:
            print(f"‚úÖ Found in cache (product {product_code}): {cached['product_name']}")
            return cached
        
        # Fetch from website
        print(f"üîç Fetching from J.Crew: {product_url}")
        product_data = self._scrape_product(product_url)
        
        if product_data:
            # Add cache metadata
            product_data['cache_key'] = cache_key
            product_data['product_code'] = product_code
            product_data['original_url'] = product_url
            
            # Save to cache using normalized key
            self._save_to_cache_by_key(cache_key, product_data)
            print(f"‚úÖ Cached new product (code {product_code}): {product_data['product_name']}")
        
        return product_data
    
    def _is_supported_product(self, product_url: str) -> bool:
        """Check if the product URL is for a supported category"""
        url_lower = product_url.lower()
        
        # Must be men's product
        if "/mens/" not in url_lower and "/men/" not in url_lower:
            return False
        
        # Supported categories - ALL men's tops (J.Crew uses ONE guide for all)
        supported_categories = [
            "/shirts/",
            "/dress-shirts/",  # ‚úÖ Added for dress shirt URLs
            "/denim-shirts/",  # ‚úÖ Added for denim shirt URLs
            "/t-shirts/",
            "/tshirts/",
            "/tshirts-and-polos/",  # ‚úÖ Added for URLs like /tshirts-and-polos/t-shirt/
            "/polos/",
            "/sweaters/",
            "/sweatshirts/",
            "/hoodies/",
            "/jacket",   # ‚úÖ Now supported - same size guide
            "/coat",     # ‚úÖ Now supported - same size guide  
            "/outerwear/",  # ‚úÖ Now supported - same size guide
            "/blazer"    # ‚úÖ Now supported - same size guide
        ]
        
        # Check if URL contains any supported category
        return any(category in url_lower for category in supported_categories)
    
    def _check_cache(self, product_url: str) -> Optional[Dict]:
        """Check if product exists in cache"""
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        
        try:
            cur.execute("""
                SELECT product_name, product_code, product_image,
                       category, subcategory, sizes_available,
                       colors_available, material, fit_type, fit_options, price,
                       product_description, fit_details
                FROM jcrew_product_cache
                WHERE product_url = %s
            """, (product_url,))
            
            row = cur.fetchone()
            if row:
                return {
                    'product_url': product_url,
                    'product_name': row[0],
                    'product_code': row[1],
                    'product_image': row[2],
                    'category': row[3],
                    'subcategory': row[4],
                    'sizes_available': row[5],
                    'colors_available': row[6],
                    'material': row[7],
                    'fit_type': row[8],
                    'fit_options': row[9],
                    'price': float(row[10]) if row[10] else None,
                    'product_description': row[11] if len(row) > 11 else '',
                    'fit_details': row[12] if len(row) > 12 else {}
                }
        finally:
            cur.close()
            conn.close()
        
        return None
    
    def _check_cache_by_key(self, cache_key: str) -> Optional[Dict]:
        """Check if product exists in cache using normalized cache key"""
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        
        try:
            # First try to find by cache_key (new method)
            cur.execute("""
                SELECT product_name, product_code, product_image,
                       category, subcategory, sizes_available,
                       colors_available, material, fit_type, fit_options, price,
                       product_description, fit_details, product_url
                FROM jcrew_product_cache
                WHERE cache_key = %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (cache_key,))
            
            row = cur.fetchone()
            if row:
                return {
                    'product_url': row[13],  # Original URL from cache
                    'product_name': row[0],
                    'product_code': row[1],
                    'product_image': row[2],
                    'category': row[3],
                    'subcategory': row[4],
                    'sizes_available': row[5],
                    'colors_available': row[6],
                    'material': row[7],
                    'fit_type': row[8],
                    'fit_options': row[9],
                    'price': float(row[10]) if row[10] else None,
                    'product_description': row[11] if len(row) > 11 else '',
                    'fit_details': row[12] if len(row) > 12 else {},
                    'cache_key': cache_key
                }
        except Exception as e:
            print(f"Cache lookup error: {e}")
        finally:
            cur.close()
            conn.close()
        
        return None
    
    def _scrape_product(self, product_url: str) -> Optional[Dict]:
        """Scrape product data from J.Crew website"""
        try:
            # First, get the original URL with color parameters for accurate image/price extraction
            print(f"üîç Fetching product data from original URL: {product_url}")
            response_original = requests.get(product_url, headers=self.headers, timeout=10)
            response_original.raise_for_status()
            soup_original = BeautifulSoup(response_original.text, 'html.parser')
            
            # For fit options extraction, use the base URL without any parameters
            # to get all available fits, not just the selected one
            base_url = product_url
            soup_base = soup_original  # Default to original if no parameters
            
            if '?' in product_url:
                import urllib.parse
                parsed = urllib.parse.urlparse(product_url)
                # Use just the base URL without any query parameters
                base_url = urllib.parse.urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    '', '', ''
                ))
                print(f"üîç Using clean base URL for fit extraction: {base_url}")
                
                # Only fetch base URL if it's different from original
                if base_url != product_url:
                    response_base = requests.get(base_url, headers=self.headers, timeout=10)
                    response_base.raise_for_status()
                    soup_base = BeautifulSoup(response_base.text, 'html.parser')
            
            # Detect category from URL
            category_info = self._detect_category(product_url)
            
            # Extract product data - use original soup for color-specific data, base soup for fit options
            product_data = {
                'product_url': product_url,
                'product_name': self._extract_name(soup_original),
                'product_code': self._extract_code(product_url, soup_original),
                'product_image': self._extract_image(soup_original),  # Use original for correct color image
                'price': self._extract_price(soup_original),  # Use original for correct color price
                'sizes_available': self._extract_sizes(soup_original),
                'colors_available': self._extract_colors(soup_original),  # Use original to get all colors
                'material': self._extract_material(soup_original),
                'fit_type': self._extract_fit(soup_original),
                'fit_options': self._extract_fit_options(soup_base, base_url),  # Use base for all fit options
                'product_description': self._extract_description(soup_original),
                'fit_details': self._extract_fit_details(soup_original),
                'category': category_info['category'],
                'subcategory': category_info['subcategory']
            }
            
            return product_data
            
        except Exception as e:
            print(f"‚ùå Error scraping {product_url}: {e}")
            # Return minimal data as fallback
            category_info = self._detect_category(product_url)
            return {
                'product_url': product_url,
                'product_name': self._extract_name_from_url(product_url),
                'product_code': self._extract_code_from_url(product_url),
                'product_image': '',
                'price': None,
                'sizes_available': ['XS', 'S', 'M', 'L', 'XL', 'XXL'],
                'colors_available': ['Default'],
                'material': '',
                'fit_type': 'Regular',
                'category': category_info['category'],
                'subcategory': category_info['subcategory']
            }
    
    def _detect_category(self, url: str) -> Dict[str, str]:
        """Detect product category and subcategory from URL"""
        url_lower = url.lower()
        
        # Sweaters & Sweatshirts (we have t-shirt guide which works for these)
        if '/sweaters/' in url_lower:
            return {'category': 'Sweaters', 'subcategory': 'Pullover'}
        elif any(x in url_lower for x in ['/sweatshirts/', '/hoodies/']):
            return {'category': 'Sweaters', 'subcategory': 'Sweatshirts'}
        
        # T-Shirts & Polos
        elif '/t-shirts/' in url_lower or '/tshirts/' in url_lower:
            return {'category': 'T-Shirts', 'subcategory': 'Short Sleeve'}
        elif '/polos/' in url_lower:
            return {'category': 'T-Shirts', 'subcategory': 'Polos'}
        
        # Shirts (default for other tops)
        elif '/shirts/' in url_lower or '/denim-shirts/' in url_lower:
            if 'casual' in url_lower or 'denim' in url_lower:
                return {'category': 'Shirts', 'subcategory': 'Casual'}
            elif 'dress' in url_lower:
                return {'category': 'Shirts', 'subcategory': 'Dress'}
            elif 'oxford' in url_lower:
                return {'category': 'Shirts', 'subcategory': 'Oxford'}
            else:
                return {'category': 'Shirts', 'subcategory': 'Casual'}
        
        # Default for men's tops
        else:
            return {'category': 'Shirts', 'subcategory': 'Casual'}
    
    def _extract_name(self, soup: BeautifulSoup) -> str:
        """Extract product name"""
        # Try multiple selectors
        selectors = [
            'h1.product-name',
            'h1[data-qaid="pdpProductName"]',
            'h1.product__name',
            'meta[property="og:title"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                if selector.startswith('meta'):
                    return element.get('content', '').strip()
                else:
                    return element.text.strip()
        
        return "J.Crew Product"
    
    def _extract_name_from_url(self, url: str) -> str:
        """Extract product name from URL as fallback"""
        # For URLs like: /p/mens/categories/clothing/shirts/casual/broken-in-oxford-shirt/BE996
        parts = url.split('/')
        for part in reversed(parts):
            if part and not part.startswith('?') and len(part) > 5:
                # Skip product codes (usually short uppercase)
                if not (len(part) < 10 and part.isupper()):
                    # Convert URL slug to readable name
                    name = part.replace('-', ' ').title()
                    return name
        return "J.Crew Shirt"
    
    def _extract_code(self, url: str, soup: BeautifulSoup) -> str:
        """Extract product code from URL or page"""
        # First try URL
        code = self._extract_code_from_url(url)
        if code:
            return code
        
        # Try from page
        sku_element = soup.select_one('[data-qaid="pdpItemNumber"]')
        if sku_element:
            return sku_element.text.strip()
        
        return ""
    
    def _extract_code_from_url(self, url: str) -> str:
        """Extract product code from URL"""
        # Look for pattern like /BE996 or /p/BE996
        match = re.search(r'/([A-Z]{2}\d{3,4})(?:\?|$|/)', url)
        if match:
            return match.group(1)
        
        # Try last segment
        parts = url.rstrip('/').split('/')
        if parts:
            last = parts[-1].split('?')[0]
            if len(last) <= 8 and any(c.isdigit() for c in last):
                return last
        
        return ""
    
    def _extract_image(self, soup: BeautifulSoup) -> str:
        """Extract main product image"""
        # Try multiple selectors
        selectors = [
            'img.product__image',
            'img[data-qaid="pdpMainImage"]',
            'meta[property="og:image"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                if selector.startswith('meta'):
                    img_url = element.get('content', '')
                else:
                    img_url = element.get('src', '')
                
                # Make URL absolute
                if img_url and not img_url.startswith('http'):
                    img_url = f"https://www.jcrew.com{img_url}"
                
                return img_url
        
        return ""
    
    def _extract_price(self, soup: BeautifulSoup) -> Optional[float]:
        """Extract product price"""
        price_element = soup.select_one('[data-qaid="pdpSalePrice"], .product__price--sale, .product__price')
        if price_element:
            price_text = price_element.text.strip()
            # Extract number from price text
            match = re.search(r'[\d,]+\.?\d*', price_text)
            if match:
                return float(match.group().replace(',', ''))
        return None
    
    def _extract_sizes(self, soup: BeautifulSoup) -> list:
        """Extract available sizes"""
        sizes = []
        
        # Try to find size buttons
        size_elements = soup.select('[data-qaid*="size"], .size-selector__button, button[aria-label*="Size"]')
        for element in size_elements:
            size_text = element.text.strip()
            if size_text and size_text not in sizes:
                sizes.append(size_text)
        
        # Default sizes if none found
        if not sizes:
            sizes = ['XS', 'S', 'M', 'L', 'XL', 'XXL']
        
        return sizes
    
    def _extract_colors(self, soup: BeautifulSoup) -> list:
        """Extract available colors with visual information from J.Crew"""
        colors = []
        
        # Target multiple possible DOM patterns used by J.Crew
        selector_list = [
            '.js-product__color.colors-list__item',
            '.js-product__color',
            '.ProductPriceColors__color',
            '[data-qaid^="pdpProductPriceColorsGroupListItem"]'
        ]
        jcrew_color_elements = soup.select(', '.join(selector_list))
        
        for element in jcrew_color_elements:
            # Extract color information from J.Crew's data attributes
            color_name = (element.get('data-name') or '').strip()
            if not color_name:
                # Fallback to aria-label like "NAVY $39.50"
                aria = (element.get('aria-label') or '').strip()
                if aria:
                    color_name = aria.split('$')[0].strip()
            color_code = (element.get('data-code') or '').strip()  # e.g., WT0002
            product_code = (element.get('data-product') or '').strip()  # e.g., BW379
            
            if not color_name:
                continue
            
            # Clean up name casing
            color_name = color_name.replace(' undefined', '').strip().title()
            
            # Skip duplicates
            existing_names = [c.get('name', '') if isinstance(c, dict) else str(c) for c in colors]
            if color_name in existing_names:
                continue
            
            color_info = {
                'name': color_name,
                'code': color_code if color_code else None,
                'productCode': product_code if product_code else None
            }
            
            # Try to extract image URL for this color (img src or data-src/srcset)
            img_element = element.find('img')
            img_src = ''
            if img_element:
                img_src = img_element.get('src') or img_element.get('data-src') or ''
                if not img_src:
                    srcset = img_element.get('srcset') or ''
                    if srcset:
                        # Take the first URL from srcset
                        img_src = srcset.split(',')[0].strip().split(' ')[0]
            if img_src:
                if img_src.startswith('/'):
                    img_src = f"https://www.jcrew.com{img_src}"
                color_info['imageUrl'] = img_src
            
            # Also attempt to read a background color if present
            style = element.get('style', '')
            if 'background-color:' in style:
                import re
                hex_match = re.search(r'background-color:\s*#([0-9a-fA-F]{6})', style)
                if hex_match:
                    color_info['hex'] = f"#{hex_match.group(1)}"
                else:
                    rgb_match = re.search(r'background-color:\s*rgb\((\d+),\s*(\d+),\s*(\d+)\)', style)
                    if rgb_match:
                        r, g, b = rgb_match.groups()
                        color_info['hex'] = f"#{int(r):02x}{int(g):02x}{int(b):02x}"
            
            colors.append(color_info)
            print(f"üé® Found color: {color_name} (code: {color_code})")
        
        print(f"üé® Total colors extracted: {len(colors)}")
        
        # Fallback: try older selectors if J.Crew specific didn't work
        if not colors:
            color_elements = soup.select('[data-qaid*="color"], .color-selector__button, button[aria-label*="Color"]')
            for element in color_elements:
                color_text = element.get('aria-label', '') or element.text.strip()
                if color_text and len(color_text) < 50:
                    # Check if this color name already exists
                    existing_names = [c['name'] if isinstance(c, dict) else c for c in colors]
                    if color_text not in existing_names:
                        colors.append({'name': color_text.title()})
        
        # Return default if no colors found
        if not colors:
            colors = [{'name': 'Default'}]
        
        return colors
    
    def _extract_material(self, soup: BeautifulSoup) -> str:
        """Extract material/fabric information"""
        # Look in product details
        details = soup.select('.product-details__content li, .pdp-details li')
        for detail in details:
            text = detail.text.lower()
            if 'cotton' in text or 'polyester' in text or 'wool' in text:
                return detail.text.strip()
        
        return ""
    
    def _extract_fit(self, soup: BeautifulSoup) -> str:
        """Extract fit type"""
        # Look for fit information
        fit_element = soup.select_one('[data-qaid*="fit"], .product__fit')
        if fit_element:
            return fit_element.text.strip()
        
        # Check in title or description
        text = soup.text.lower()
        if 'slim' in text:
            return 'Slim'
        elif 'classic' in text or 'regular' in text:
            return 'Classic'
        elif 'relaxed' in text:
            return 'Relaxed'
        
        return 'Regular'
    
    def _extract_fit_options(self, soup: BeautifulSoup, product_url: str) -> list:
        """Extract available fit options for this specific product"""
        fit_options = []
        
        # Determine if this is a men's or women's product
        is_mens_product = '/mens/' in product_url.lower() or '/men/' in product_url.lower()
        
        # Method 1: Look for ProductVariations section (most accurate for J.Crew)
        product_variations = soup.find('div', {'id': 'c-product__variations'})
        if product_variations:
            # Look for fit variation buttons within the ProductVariations section
            fit_buttons = product_variations.find_all('button', {'class': lambda x: x and 'js-product_variation' in x})
            for button in fit_buttons:
                # Check if this is a fit variation (not size or color)
                data_label = button.get('data-label', '').strip()
                if data_label and any(fit_word in data_label.lower() for fit_word in ['classic', 'slim', 'tall', 'relaxed', 'untucked']):
                    if data_label not in fit_options:
                        fit_options.append(data_label)
                        print(f"üéØ Found fit option in ProductVariations: {data_label}")
        
        # Method 1.5: Look for fit variation lists or groups (alternative J.Crew structure)
        if not fit_options:
            # Look for fit variation lists
            fit_lists = soup.find_all('ul', {'class': lambda x: x and 'variations-list' in x})
            for fit_list in fit_lists:
                fit_items = fit_list.find_all('li', {'class': lambda x: x and 'js-product_variation' in x})
                for item in fit_items:
                    data_label = item.get('data-label', '').strip()
                    if data_label and any(fit_word in data_label.lower() for fit_word in ['classic', 'slim', 'tall', 'relaxed', 'untucked']):
                        if data_label not in fit_options:
                            fit_options.append(data_label)
                            print(f"üéØ Found fit option in variations list: {data_label}")
            
            # Also check for any elements with fit-related data attributes
            fit_elements = soup.find_all(attrs={'data-fit': True})
            for element in fit_elements:
                fit_value = element.get('data-fit', '').strip()
                if fit_value and fit_value not in fit_options:
                    fit_options.append(fit_value.title())
                    print(f"üéØ Found fit option in data-fit attribute: {fit_value}")
        
        # Method 2: Look for actual fit selector buttons on the page (fallback)
        if not fit_options:
            fit_selectors = [
                'button[data-testid*="fit"]',
                'button[aria-label*="fit"]',
                '.fit-selector button',
                '[data-fit-type]'
            ]
            
            for selector in fit_selectors:
                elements = soup.select(selector)
                for element in elements:
                    fit_text = element.get_text().strip()
                    aria_label = element.get('aria-label', '').strip()
                    
                    for text_source in [fit_text, aria_label]:
                        if text_source and text_source not in fit_options:
                            # Common J.Crew fit types
                            if any(fit in text_source.lower() for fit in ['classic', 'slim', 'tall', 'relaxed', 'untucked']):
                                fit_options.append(text_source)
        
        # Method 2: Extract from JSON-LD structured data (fallback, but filter carefully)
        # DISABLED: This method is too unreliable - it finds fits in metadata that aren't actually available
        # if not fit_options:
        #     scripts = soup.find_all('script')
        #     for script in scripts:
        #         if script.string and 'fit=' in script.string:
        #             script_content = script.string
        #             # Look for all URLs with fit parameters
        #             fit_urls = re.findall(r'fit=([^&"]*)', script_content)
        #             if fit_urls:
        #                 for fit in fit_urls:
        #                     if fit and fit not in fit_options:
        #                         # Filter out inappropriate fit options based on gender and product type
        #                         if is_mens_product and fit.lower() == 'petite':
        #                             print(f"‚ö†Ô∏è Skipping 'Petite' fit option for men's product")
        #                             continue  # Skip "Petite" for men's products
        #                         
        #                         # Allow all valid fit options for all products including T-shirts
        #                         # J.Crew T-shirts do have Classic and Tall fit variations
        #                         
        #                         fit_options.append(fit)
        #                 
        #                 if fit_options:
        #                     print(f"üéØ Found fit options in JSON data (filtered for {'mens' if is_mens_product else 'womens'}): {fit_options}")
        #                 else:
        #                     print(f"üéØ No valid fit options found in JSON data for this product")
        #                 break
        
        # Method 2: Look for fit selector buttons on the page (fallback)
        if not fit_options:
            fit_selectors = [
                'button[data-testid*="fit"]',
                'button[aria-label*="fit"]',
                'button[data-testid*="Fit"]',
                'button[aria-label*="Fit"]',
                '.fit-selector button',
                'button[class*="fit"]',
                'button[class*="Fit"]',
                # More generic selectors for J.Crew's current structure
                'button[aria-label*="Classic"]',
                'button[aria-label*="Tall"]',
                'button[aria-label*="Slim"]',
                'button[aria-label*="Relaxed"]',
                'button[data-testid*="Classic"]',
                'button[data-testid*="Tall"]',
                'button[data-testid*="Slim"]',
                'button[data-testid*="Relaxed"]'
            ]
            
            for selector in fit_selectors:
                elements = soup.select(selector)
                for element in elements:
                    # Check both text content and aria-label
                    fit_text = element.get_text().strip()
                    aria_label = element.get('aria-label', '').strip()
                    
                    for text_source in [fit_text, aria_label]:
                        if text_source and text_source not in fit_options:
                            # Common J.Crew fit types
                            if any(fit in text_source.lower() for fit in ['classic', 'slim', 'tall', 'relaxed', 'untucked']):
                                fit_options.append(text_source)
        
        # Method 3: Look for buttons that might contain fit information (broader search)
        if not fit_options:
            all_buttons = soup.select('button')
            for button in all_buttons:
                button_text = button.get_text().strip().lower()
                aria_label = button.get('aria-label', '').strip().lower()
                
                # Check if button contains fit-related text
                for text_source in [button_text, aria_label]:
                    if any(fit_word in text_source for fit_word in ['classic', 'tall', 'slim', 'relaxed']):
                        # Extract the fit name
                        if 'classic' in text_source:
                            if 'Classic' not in fit_options:
                                fit_options.append('Classic')
                        if 'tall' in text_source:
                            if 'Tall' not in fit_options:
                                fit_options.append('Tall')
                        if 'slim' in text_source and 'untucked' not in text_source:
                            if 'Slim' not in fit_options:
                                fit_options.append('Slim')
                        if 'relaxed' in text_source:
                            if 'Relaxed' not in fit_options:
                                fit_options.append('Relaxed')
        
        # Method 4: Check URL parameters for fit options (if user is on a specific fit page)
        # BUT ONLY if we haven't found any other fit options - URL params alone don't indicate multiple options
        if not fit_options and 'fit=' in product_url:
            import urllib.parse
            parsed = urllib.parse.urlparse(product_url)
            params = urllib.parse.parse_qs(parsed.query)
            if 'fit' in params:
                current_fit = params['fit'][0]
                # Only add if it's a meaningful fit type, not just a default
                if current_fit.lower() in ['classic', 'slim', 'tall', 'relaxed', 'untucked']:
                    print(f"‚ö†Ô∏è Found fit parameter in URL: {current_fit} - but this doesn't guarantee multiple options exist")
                    # Don't add it to fit_options yet - we need to verify multiple options exist
        
        # Method 5: Look for fit information in product details or descriptions
        if not fit_options:
            fit_text_indicators = soup.find_all(text=re.compile(r'(classic|slim|tall|relaxed|untucked)', re.I))
            for text in fit_text_indicators:
                parent = text.parent
                if parent and any(cls in parent.get('class', []) for cls in ['fit', 'size', 'product']):
                    # Extract fit types from the text
                    fits = re.findall(r'\b(Classic|Slim|Tall|Relaxed|Untucked)\b', text, re.I)
                    for fit in fits:
                        if fit.title() not in fit_options:
                            fit_options.append(fit.title())
        
        # Method 6: Check for common J.Crew fit patterns in the page
        if not fit_options:
            page_text = soup.get_text().lower()
            common_fits = ['classic', 'slim', 'slim untucked', 'tall', 'relaxed']
            
            # Look for fit selection context (e.g., "Available in Classic and Slim fits")
            if any(indicator in page_text for indicator in ['available in', 'choose your fit', 'fit options']):
                for fit in common_fits:
                    if fit in page_text and fit.title() not in fit_options:
                        fit_options.append(fit.title())
        
        # Clean up and standardize fit names
        standardized_fits = []
        for fit in fit_options:
            fit_clean = fit.strip().title()
            if fit_clean == 'Slim Untucked':
                fit_clean = 'Slim Untucked'
            elif 'untucked' in fit_clean.lower():
                fit_clean = 'Slim Untucked'
            
            if fit_clean not in standardized_fits:
                standardized_fits.append(fit_clean)
        
        # Final validation: Be smarter about returning fit options
        
        # Check for actual fit selection UI elements (broader search)
        actual_fit_selectors = soup.select(
            'button[data-testid*="fit"], '
            'button[aria-label*="fit"], '
            '.fit-selector button, '
            '[data-fit-type], '
            '.product__fit-option, '
            '[data-qaid*="fitOption"], '
            'a[href*="fit="], '  # Links with fit parameter
            'input[name*="fit"], '  # Form inputs for fit
            'select[name*="fit"], '  # Dropdown for fit
            '[class*="fit-option"], '  # Any element with fit-option class
            '[id*="fit-option"]'  # Any element with fit-option id
        )
        
        # Also check if we're on a product that typically has fit options (dress shirts, suits, etc)
        is_dress_shirt = 'dress-shirt' in product_url.lower() or 'bowery' in product_url.lower() or 'ludlow' in product_url.lower()
        is_formal_wear = 'suit' in product_url.lower() or 'tuxedo' in product_url.lower() or 'blazer' in product_url.lower()
        
        # Check if the page explicitly mentions multiple fits in text
        page_text = soup.get_text().lower()
        has_fit_mentions = any(phrase in page_text for phrase in [
            'available in classic, slim',
            'classic and slim fit',
            'classic, slim, and tall',
            'choose your fit',
            'select fit'
        ])
        
        # If we're on a dress shirt or formal wear page, these ALWAYS have fit options at J.Crew
        if is_dress_shirt or is_formal_wear:
            # Look for the common J.Crew dress shirt fits
            common_dress_shirt_fits = ['Classic', 'Slim', 'Tall']
            if not standardized_fits or len(standardized_fits) <= 1:
                # Dress shirts and formal wear always have these fits at J.Crew
                print(f"üìã Detected dress shirt/formal wear, using standard J.Crew fits")
                standardized_fits = common_dress_shirt_fits
            
        # If no actual fit selection UI elements exist and we're not on a known fit product, don't return
        if not actual_fit_selectors and not (is_dress_shirt or is_formal_wear or has_fit_mentions):
            print(f"üö´ No fit selection UI found and not a known fit product. Not returning: {standardized_fits}")
            return []
        
        # Require multiple fit options - a single fit means no choice
        if len(standardized_fits) <= 1:
            # For dress shirts, if we only found one but URL has fit parameter, add common options
            if (is_dress_shirt or is_formal_wear) and 'fit=' in product_url:
                print(f"üìã Dress shirt with single fit, adding standard options")
                standardized_fits = ['Classic', 'Slim', 'Tall']
            else:
                print(f"üö´ Only found {len(standardized_fits)} fit option(s): {standardized_fits}. No variations exist.")
                return []
        
        print(f"‚úÖ Found {len(standardized_fits)} fit options: {standardized_fits}")
        return standardized_fits
    
    def _save_to_cache(self, product_data: Dict):
        """Save product data to cache"""
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        
        try:
            cur.execute("""
                INSERT INTO jcrew_product_cache (
                    product_url, product_code, product_name, product_image,
                    category, subcategory, price, sizes_available,
                    colors_available, material, fit_type, fit_options, 
                    product_description, fit_details, created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                ON CONFLICT (product_url) DO UPDATE SET
                    product_name = EXCLUDED.product_name,
                    product_image = EXCLUDED.product_image,
                    price = EXCLUDED.price,
                    sizes_available = EXCLUDED.sizes_available,
                    fit_options = EXCLUDED.fit_options,
                    product_description = EXCLUDED.product_description,
                    fit_details = EXCLUDED.fit_details,
                    updated_at = NOW()
            """, (
                product_data['product_url'],
                product_data.get('product_code', ''),
                product_data['product_name'],
                product_data.get('product_image', ''),
                product_data.get('category', 'Shirts'),
                product_data.get('subcategory', 'Casual'),
                product_data.get('price'),
                product_data.get('sizes_available', []),
                # Convert color dicts to strings
                [c['name'] if isinstance(c, dict) else c for c in product_data.get('colors_available', [])],
                product_data.get('material', ''),
                product_data.get('fit_type', 'Regular'),
                product_data.get('fit_options', []),
                product_data.get('product_description', ''),
                json.dumps(product_data.get('fit_details', {}))
            ))
            
            conn.commit()
        except Exception as e:
            print(f"‚ùå Error saving to cache: {e}")
            conn.rollback()
        finally:
            cur.close()
            conn.close()
    
    def _save_to_cache_by_key(self, cache_key: str, product_data: Dict):
        """Save product data to cache using normalized cache key"""
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        
        try:
            # First, check if cache_key column exists, if not add it
            cur.execute("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'jcrew_product_cache' AND column_name = 'cache_key'
            """)
            
            if not cur.fetchone():
                print("üîß Adding cache_key column to jcrew_product_cache table...")
                cur.execute("""
                    ALTER TABLE jcrew_product_cache 
                    ADD COLUMN IF NOT EXISTS cache_key VARCHAR(255)
                """)
                cur.execute("""
                    CREATE UNIQUE INDEX IF NOT EXISTS idx_jcrew_cache_key_unique 
                    ON jcrew_product_cache(cache_key)
                """)
                conn.commit()
            
            # Insert or update with cache_key
            cur.execute("""
                INSERT INTO jcrew_product_cache (
                    product_url, product_code, product_name, product_image,
                    category, subcategory, price, sizes_available,
                    colors_available, material, fit_type, fit_options, 
                    product_description, fit_details, cache_key, created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                ON CONFLICT (cache_key) DO UPDATE SET
                    product_url = EXCLUDED.product_url,
                    product_name = EXCLUDED.product_name,
                    product_image = EXCLUDED.product_image,
                    price = EXCLUDED.price,
                    sizes_available = EXCLUDED.sizes_available,
                    fit_options = EXCLUDED.fit_options,
                    product_description = EXCLUDED.product_description,
                    fit_details = EXCLUDED.fit_details,
                    updated_at = NOW()
            """, (
                product_data.get('original_url', product_data.get('product_url', '')),
                product_data.get('product_code', ''),
                product_data['product_name'],
                product_data.get('product_image', ''),
                product_data.get('category', 'Shirts'),
                product_data.get('subcategory', 'Casual'),
                product_data.get('price'),
                product_data.get('sizes_available', []),
                # Convert color dicts to strings
                [c['name'] if isinstance(c, dict) else c for c in product_data.get('colors_available', [])],
                product_data.get('material', ''),
                product_data.get('fit_type', 'Regular'),
                product_data.get('fit_options', []),
                product_data.get('product_description', ''),
                json.dumps(product_data.get('fit_details', {})),
                cache_key
            ))
            
            conn.commit()
            print(f"üíæ Cached product with key: {cache_key}")
            
        except Exception as e:
            print(f"‚ùå Error saving to cache with key: {e}")
            conn.rollback()
        finally:
            cur.close()
            conn.close()
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """Extract product description with fit and fabric details"""
        description_parts = []
        
        # Method 1: Look for specific product description text patterns
        # J.Crew often has description text in specific areas
        text_content = soup.get_text()
        
        # Look for common description patterns
        description_patterns = [
            r'Inspired by[^.]*\.[^.]*\.[^.]*\.',  # "Inspired by..." sentences
            r'[Tt]his [^.]*(?:cotton|fabric|fit|cut|made|designed)[^.]*\.[^.]*\.',  # "This tee is made from..."
            r'[Mm]ade from[^.]*\.[^.]*\.',  # "Made from..." sentences
            r'\d+(?:\.\d+)?[- ]ounce[^.]*\.[^.]*\.',  # Weight descriptions
            r'[Ww]ith [^.]*(?:room|fit|cut)[^.]*\.[^.]*\.',  # Fit descriptions
        ]
        
        for pattern in description_patterns:
            matches = re.findall(pattern, text_content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                # Clean up the match
                clean_match = re.sub(r'\s+', ' ', match.strip())
                if len(clean_match) > 30 and clean_match not in description_parts:
                    description_parts.append(clean_match)
        
        # Method 2: Look for material and construction details
        material_patterns = [
            r'100% [^.]*\.',  # "100% cotton."
            r'[Rr]ib trim[^.]*\.',  # "Rib trim at neck."
            r'[Ss]hort sleeves\.',  # "Short sleeves."
            r'[Mm]achine wash\.',  # "Machine wash."
            r'[Ii]mported\.',  # "Imported."
        ]
        
        for pattern in material_patterns:
            matches = re.findall(pattern, text_content)
            for match in matches:
                clean_match = match.strip()
                if clean_match not in description_parts:
                    description_parts.append(clean_match)
        
        # Method 3: Look for fit-specific information
        fit_patterns = [
            r'[Ff]its? true to size[^.]*\.',  # "Fits true to size..."
            r'[Ss]leeves? (?:are )?[^.]*(?:longer|shorter)[^.]*\.',  # Sleeve length info
            r'[Mm]ore room (?:across|in)[^.]*\.',  # Room descriptions
        ]
        
        for pattern in fit_patterns:
            matches = re.findall(pattern, text_content)
            for match in matches:
                clean_match = match.strip()
                if clean_match not in description_parts:
                    description_parts.append(clean_match)
        
        # Combine and clean up
        if description_parts:
            # Join with spaces and limit length
            full_description = ' '.join(description_parts)
            # Remove excessive whitespace
            full_description = re.sub(r'\s+', ' ', full_description)
            # Limit to reasonable length (500 chars)
            if len(full_description) > 500:
                full_description = full_description[:500] + '...'
            return full_description
        
        return ""
    
    def _extract_fit_details(self, soup: BeautifulSoup) -> dict:
        """Extract specific fit details like model info, sizing notes, etc."""
        fit_details = {}
        
        # Look for model information
        model_info = soup.find(text=re.compile(r'Model is.*wearing', re.I))
        if model_info:
            fit_details['model_info'] = model_info.strip()
        
        # Look for fit information
        fit_info_selectors = [
            '[data-testid="size-fit"]',
            '.size-fit-info',
            '.fit-information'
        ]
        
        for selector in fit_info_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if 'fit' in text.lower():
                    fit_details['fit_notes'] = text
        
        # Look for customer review summary
        review_text = soup.find(text=re.compile(r'based on.*customer reviews', re.I))
        if review_text:
            fit_details['customer_feedback'] = review_text.strip()
        
        # Look for specific fit notes (like sleeve length differences)
        fit_notes = soup.find_all(text=re.compile(r'(longer|shorter|different|due to|because of)', re.I))
        for note in fit_notes:
            if any(keyword in note.lower() for keyword in ['sleeve', 'fit', 'length', 'size']):
                if 'fit_notes' not in fit_details:
                    fit_details['fit_notes'] = []
                elif isinstance(fit_details['fit_notes'], str):
                    fit_details['fit_notes'] = [fit_details['fit_notes']]
                
                if isinstance(fit_details['fit_notes'], list):
                    fit_details['fit_notes'].append(note.strip())
        
        return fit_details


# Test function
if __name__ == "__main__":
    # Test with some J.Crew URLs
    test_urls = [
        "https://www.jcrew.com/p/mens/categories/clothing/shirts/casual/BH290",
        "https://www.jcrew.com/p/mens/categories/clothing/shirts/broken-in-oxford/broken-in-organic-cotton-oxford-shirt/BE996",
    ]
    
    fetcher = JCrewProductFetcher()
    
    for url in test_urls:
        print(f"\n{'='*60}")
        print(f"Testing: {url}")
        print('='*60)
        product = fetcher.fetch_product(url)
        if product:
            print(f"‚úÖ Name: {product['product_name']}")
            print(f"‚úÖ Code: {product['product_code']}")
            print(f"‚úÖ Image: {product['product_image'][:80] if product['product_image'] else 'No image'}...")
            print(f"‚úÖ Sizes: {product['sizes_available']}")
            print(f"‚úÖ Price: ${product['price']}" if product['price'] else "Price not found")
        else:
            print(f"‚ùå Failed to fetch product data")
